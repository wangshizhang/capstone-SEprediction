{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bd1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "# import torch.tensor as tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torchvision import models\n",
    "#from resnet import resnet50\n",
    "#from res2net import res2net50_48w_2s,res2net50_26w_8s,res2net50\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from resnet import resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b52663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor(x):\n",
    "    return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d6e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[:,0:16] dataset_pcell_csi_rsrp\n",
    "# X[:,17:32]dataset_pcell_csi_rsrp_std\n",
    "# X[:,33:34]dataset_ri\n",
    "# X[:,34:35]dataset_mcs\n",
    "# X[:,35:36]dataset_cqi\n",
    "# X[:,36:42]dataset_ncell_ss_rsrp\n",
    "# X[:,42:47]dataset_ncell_ss_rsrp_std\n",
    "\n",
    "# y[:,0] SE  \n",
    "# y[:,1] (I+N)a\n",
    "# y[:,2] (I+N)b\n",
    "# y[:,3] SINR(CQI)\n",
    "# y是归一化后的数据，归一化方式，y=y_original/y_max\n",
    "#后面提供y_max是为了方便将归一化后的y_true和y_pred还原\n",
    "# y_max[0] max(y[:,0]) \n",
    "# y_max[1] min(y[:,1])\n",
    "# y_max[2] min(y[:,2])\n",
    "# y_max[3] max(y[:,3]) \n",
    "\n",
    "with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl','rb') as f:\n",
    "    X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913b14d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814140, 48) (814140, 4) (83192, 48) (83192, 4) (102525, 48) (102525, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719a26b",
   "metadata": {},
   "source": [
    "### Dataset process for label training - split original dataset and transfer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b647fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_transfer(labels,transfer_pairs):\n",
    "    for pair in transfer_pairs:\n",
    "        print(labels==pair[0])\n",
    "        labels[labels==pair[0]] = pair[1]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c9fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2label(y_ori,y_max):\n",
    "    labels = np.zeros(y_ori.shape).astype(np.int32)+3\n",
    "    \n",
    "    labels[y_ori*y_max <= 2250] = 2\n",
    "    labels[y_ori*y_max <= 1850] = 1\n",
    "    labels[y_ori*y_max <= 1350] = 0\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373b9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train[:,0:32],X_train[:,36:42]))\n",
    "y_train = y_train[:,0]\n",
    "X_val = np.hstack((X_val[:,0:32],X_val[:,36:42]))\n",
    "y_val = y_val[:,0]\n",
    "X_test = np.hstack((X_test[:,0:32],X_test[:,36:42]))\n",
    "y_test = y_test[:,0]\n",
    "ylabel_train = y2label(y_train,y_max[0])\n",
    "ylabel_val = y2label(y_val,y_max[0])\n",
    "ylabel_test = y2label(y_test,y_max[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814140, 38) (814140,) (83192, 38) (83192,) (102525, 38) (102525,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e005fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [sum(ylabel_train == 0),sum(ylabel_train == 1),sum(ylabel_train == 2),sum(ylabel_train == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05666327, 0.05970148, 0.06275808, ..., 0.27547748, 0.26365272,\n",
       "       0.25167538])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f412932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(counts)),counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16f53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_train = np.hstack((X_train[:,0:32],X_train[:,36:42]))\n",
    "        self.y_train = y_train[:,0]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_train = tensor(self.X_train[idx,:])\n",
    "        label = tensor(self.y_train[idx])\n",
    "    \n",
    "        return x_train,label\n",
    "\n",
    "class Val_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_val = np.hstack((X_val[:,0:32],X_val[:,36:42]))\n",
    "        self.y_val = y_val[:,0]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_val.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_val = tensor(self.X_val[idx,:])\n",
    "        label = tensor(self.y_val[idx])\n",
    "    \n",
    "        return x_val,label\n",
    "\n",
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_test = np.hstack((X_test[:,0:32],X_test[:,36:42]))\n",
    "        self.y_test = y_test[:,0]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_test.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_test = tensor(self.X_test[idx,:])\n",
    "        label = tensor(self.y_test[idx])\n",
    "    \n",
    "        return x_test,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be607440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Label_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_train = np.hstack((X_train[:,0:32],X_train[:,36:42]))\n",
    "        self.y_train = y2label(y_train[:,0],y_max[0])\n",
    "        \n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_train = tensor(self.X_train[idx,:])\n",
    "        label = tensor(self.y_train[idx])\n",
    "    \n",
    "        return x_train,label\n",
    "\n",
    "class Val_Label_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_val = np.hstack((X_val[:,0:32],X_val[:,36:42]))\n",
    "        self.y_val = y2label(y_val[:,0],y_max[0])\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_val.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_val = tensor(self.X_val[idx,:])\n",
    "        label = tensor(self.y_val[idx])\n",
    "    \n",
    "        return x_val,label\n",
    "\n",
    "class Test_Label_Dataset(Dataset):\n",
    "    def __init__(self, path=\"DataSet_2021_9_6_filter_combine_normalize_cd.pkl\",transform=None, target_transform=None):\n",
    "        \n",
    "        with open('DataSet_2021_9_6_filter_combine_normalize_cd.pkl', 'rb') as f:\n",
    "            X_train,y_train,X_val,y_val,X_test,y_test,y_max,ff = pickle.load(f)\n",
    "        \n",
    "        self.X_test = np.hstack((X_test[:,0:32],X_test[:,36:42]))\n",
    "        self.y_test = y2label(y_test[:,0],y_max[0])\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_test.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #image = torchvision.transforms.functional.to_tensor(np.array(image.resize((240,320),Image.BILINEAR)))\n",
    "        #label = tensor(self.label2vec(label))\n",
    "        \n",
    "        x_test = tensor(self.X_test[idx,:])\n",
    "        label = tensor(self.y_test[idx])\n",
    "    \n",
    "        return x_test,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843a6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, classifier, criterion, optimizer,show_interval = 3000):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    acc = []\n",
    "    # f1_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = torch.tensor(np.array(labels))\n",
    "        images, labels = images.to(torch.float32).to(device), labels.to(torch.int64).to(device)\n",
    "        \n",
    "        logits = classifier(images)\n",
    "        loss = criterion(logits,labels)\n",
    "        loss = loss.requires_grad_()\n",
    "        \n",
    "        if i % show_interval == 0:\n",
    "            print(i*args.batch_size)\n",
    "            # acc_new = acc_cal(logits.detach(),labels.numpy())\n",
    "            acc_new = acc_f1_cal_multi(logits.detach(),labels.numpy())\n",
    "            acc += np.mean(acc_new)\n",
    "            # f1_list += [f1]\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach())\n",
    "        \n",
    "    return torch.stack(losses).mean().item(),np.mean(acc)\n",
    "\n",
    "def train_classifier_cnn(train_loader, classifier, criterion, optimizer,show_interval = 3000):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    acc = []\n",
    "    # f1_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = torch.tensor(np.array(labels))\n",
    "        images, labels = images.to(torch.float32).to(device), labels.to(torch.int64).to(device)\n",
    "        \n",
    "        images = images.reshape((images.shape[0],1,images.shape[1]))\n",
    "        logits = classifier(images)\n",
    "        logits = logits.reshape((logits.shape[0],logits.shape[2]))\n",
    "        loss = criterion(logits,labels)\n",
    "        loss = loss.requires_grad_()\n",
    "        \n",
    "        if i % show_interval == 0:\n",
    "            print(i*args.batch_size)\n",
    "            # acc_new = acc_cal(logits.detach(),labels.numpy())\n",
    "            # acc_new,_ = acc_f1_cal_multi(logits.detach(),labels.numpy())\n",
    "            # acc += np.mean(acc_new)\n",
    "            # f1_list += [f1]\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach())\n",
    "        \n",
    "    return torch.stack(losses).mean().item()\n",
    "    \n",
    "def test_classifier(test_loader, classifier, criterion, print_ind_classes=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    logits_all = np.zeros((0,4))\n",
    "    labels_all = np.zeros((0))\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            labels = torch.tensor(np.array(labels))\n",
    "            images, labels = images.to(torch.float32).to(device), labels.to(torch.int64).to(device)\n",
    "\n",
    "            logits = classifier(images)\n",
    "            # print(logits.shape,labels.shape)\n",
    "            logits_all = np.vstack((logits_all,logits.detach()))\n",
    "            labels_all = np.hstack((labels_all,labels))\n",
    "            # acc_new = acc_cal(logits,labels)\n",
    "            # acc += [acc_new]\n",
    "            \n",
    "            loss = criterion(logits,labels)\n",
    "            losses.append(loss.detach())\n",
    "        \n",
    "        test_loss = torch.stack(losses).mean().item()\n",
    "    return test_loss,acc_f1_cal_multi(logits_all,labels_all)\n",
    "\n",
    "def test_classifier_cnn(test_loader, classifier, criterion, print_ind_classes=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    logits_all = np.zeros((0,4))\n",
    "    labels_all = np.zeros((0))\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            labels = torch.tensor(np.array(labels))\n",
    "            images, labels = images.to(torch.float32).to(device), labels.to(torch.int64).to(device)\n",
    "\n",
    "            images = images.reshape((images.shape[0],1,images.shape[1]))\n",
    "            logits = classifier(images)\n",
    "            logits = logits.reshape((logits.shape[0],logits.shape[2]))\n",
    "            # print(logits.shape,labels.shape)\n",
    "            logits_all = np.vstack((logits_all,logits.detach().cpu()))\n",
    "            labels_all = np.hstack((labels_all,labels.cpu()))\n",
    "            # acc_new = acc_cal(logits,labels)\n",
    "            # acc += [acc_new]\n",
    "            \n",
    "            loss = criterion(logits,labels)\n",
    "            losses.append(loss.detach())\n",
    "        \n",
    "        test_loss = torch.stack(losses).mean().item()\n",
    "    return test_loss,acc_f1_cal_multi(logits_all,labels_all)\n",
    "\n",
    "def acc_cal(logits,labels,show = False):\n",
    "    pred_labels = np.argmax(logits,1)\n",
    "    pred_flags = pred_labels == labels\n",
    "    acc = np.sum(pred_flags)/pred_labels.shape[0]\n",
    "    if(show):\n",
    "        print(pred_labels)\n",
    "    return acc\n",
    "\n",
    "def acc_f1_cal_multi(logits,labels):\n",
    "    # print(logits.shape,labels.shape)\n",
    "    labels = labels.astype(np.int32)\n",
    "    logits = np.argmax(logits,1)\n",
    "    f1 = f1_score(logits,labels,average=None)\n",
    "    label_set = set([0,1,2,3])\n",
    "    acc = np.zeros((len(label_set)))\n",
    "    for label in label_set:\n",
    "        index = labels == label\n",
    "        print(label,np.sum(index))\n",
    "        acc[label] = np.sum(logits[index] == labels[index])/(len(labels[index]))\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf8ccf",
   "metadata": {},
   "source": [
    "### Hyper Parameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f47ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class paras():\n",
    "    def __init__(self):\n",
    "        self.batch_size = None\n",
    "        self.classes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a2bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = paras()\n",
    "args.batch_size = 128\n",
    "#args.classes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1e289",
   "metadata": {},
   "source": [
    "### Dataloader Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc90545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Train_Dataset()\n",
    "# val_dataset = Val_Dataset()\n",
    "# test_dataset = Test_Dataset()\n",
    "\n",
    "train_label_dataset = Train_Label_Dataset()\n",
    "val_label_dataset = Val_Label_Dataset()\n",
    "test_label_dataset = Test_Label_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4831a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6ea4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dataloader = DataLoader(train_label_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "val_label_dataloader = DataLoader(val_label_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_label_dataloader = DataLoader(test_label_dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ccedc",
   "metadata": {},
   "source": [
    "### Model Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ebfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "04ff3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.BN1 = nn.BatchNorm1d(2*input_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        self.BN2 = nn.BatchNorm1d(4*input_size)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.BN3 = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        self.fc4 = nn.Linear(input_size, 8)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.BN4 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        self.fc5 = nn.Linear(8, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.tensor(x).to(torch.float32)\n",
    "        x = self.BN1(F.relu(self.fc1(x)))\n",
    "        x = self.BN2(F.relu(self.fc2(x)))\n",
    "        x = self.BN3(F.relu(self.fc3(x)))\n",
    "        x = self.BN4(F.relu(self.fc4(x)))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, 8*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.fc4 = nn.Linear(8*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        \n",
    "        self.fc5 = nn.Linear(input_size, input_size//4)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        self.fc6 = nn.Linear(input_size//4, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc6.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x0 = torch.tensor(x).to(torch.float32)\n",
    "        x = nn.LeakyReLU(0.3)(self.fc1(x0))\n",
    "        x = nn.LeakyReLU(0.2)(self.fc2(x))\n",
    "        x = nn.LeakyReLU(0.18)(self.fc3(x))\n",
    "        \n",
    "        x = nn.LeakyReLU(0.15)(self.fc4(x))+x0 #residual here\n",
    "        \n",
    "        x = nn.LeakyReLU(0.1)(self.fc5(x))\n",
    "        x = torch.sigmoid(self.fc6(x))\n",
    "        \n",
    "        return x    \n",
    "class TestMLP(nn.Module):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.fc4 = nn.Linear(input_size, 8)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        \n",
    "        self.fc5 = nn.Linear(8, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.tensor(x).to(torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP1DCNN(nn.Module):\n",
    "    # 两层卷积 + MLP\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,5,padding = 2)\n",
    "        self.conv2 = nn.Conv1d(1,1,3,padding = 1)\n",
    "        self.conv3 = nn.Conv1d(1,1,7,padding = 3)\n",
    "        self.conv4 = nn.Conv1d(3,1,5,padding = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(38, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.fc4 = nn.Linear(input_size, 8)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        \n",
    "        self.fc5 = nn.Linear(8, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.tensor(x).to(torch.float32)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x_sum = torch.cat((x1,x2,x3),axis = 1)\n",
    "        x = self.conv4(x_sum)\n",
    "#         x = self.conv2(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP1DCNN_v1(nn.Module):\n",
    "    # 两层卷积 + MLP\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3,padding = 1)\n",
    "        self.conv2 = nn.Conv1d(1,1,5,padding = 2)\n",
    "        self.conv3 = nn.Conv1d(1,1,7,padding = 3)\n",
    "        # self.conv4 = nn.Conv1d(1,1,9,padding = 4)\n",
    "        # self.conv5 = nn.Conv1d(1,1,11,padding = 5)\n",
    "        \n",
    "\n",
    "        # self.conv4 = nn.Conv1d(3,1,5,padding = 2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(38, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.fc4 = nn.Linear(input_size, 8)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        \n",
    "        self.fc5 = nn.Linear(8, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.tensor(x).to(torch.float32)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x4 = self.conv4(x)\n",
    "        x5 = self.conv5(x)\n",
    "        # x_sum = torch.cat((x1,x2,x3),axis = 1)\n",
    "        # x = self.conv4(x_sum)+x\n",
    "        x = x1 + x2 + x3\n",
    "#         x = self.conv2(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP1DCNN_v2(nn.Module):\n",
    "    # 两层卷积 + MLP\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3,padding = 1)\n",
    "        self.conv2 = nn.Conv1d(1,1,5,padding = 2)\n",
    "        self.conv3 = nn.Conv1d(1,1,7,padding = 3)\n",
    "        self.conv4 = nn.Conv1d(1,1,9,padding = 4)\n",
    "        self.conv5 = nn.Conv1d(1,1,11,padding = 5)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "\n",
    "        # self.conv4 = nn.Conv1d(3,1,5,padding = 2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(38, 2*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2*input_size, 4*input_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4*input_size, input_size)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.fc4 = nn.Linear(input_size, 8)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        \n",
    "        self.fc5 = nn.Linear(8, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        #x = flatten(x)\n",
    "        #scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.tensor(x).to(torch.float32)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "        x4 = self.conv4(x)\n",
    "        x5 = self.conv5(x)\n",
    "        # x_sum = torch.cat((x1,x2,x3),axis = 1)\n",
    "        # x = self.conv4(x_sum)+x\n",
    "        x = x1 + x2 + x3\n",
    "#         x = self.conv2(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90afd88",
   "metadata": {},
   "source": [
    "model = DeepMLP(X_train.shape[1],1)\n",
    "classifier = model.to(device)\n",
    "\n",
    "criterion = torch.nn.L1Loss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "daaeffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_classifier = torch.load(\"label_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d446a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label classifier model\n",
    "model = MLP1DCNN_v2(X_train.shape[1],4)\n",
    "classifier = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad21e5",
   "metadata": {},
   "source": [
    "### Optimizer Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "869f44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "moment = 0.8\n",
    "epoch_sum = 0\n",
    "\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=lr)\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, classifier.parameters()), lr=lr, momentum=moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "74f2890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch_sum == 0:\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_f1_list = []\n",
    "    time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7b84e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CVenv\\lib\\site-packages\\ipykernel_launcher.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384000\n",
      "768000\n",
      "Loss for Training on Epoch 0 is 1.2052440643310547\n",
      "Time for epoch 0 is 57.331459283828735\n",
      "0 10949\n",
      "1 32298\n",
      "2 26425\n",
      "3 13520\n",
      "Evaluating classifier\n",
      "Loss for validation on Epoch0 is (1.3475909233093262, (array([0.        , 0.32806985, 0.53248817, 0.60747041]), array([0.        , 0.40728   , 0.43427002, 0.42555507])))\n",
      "Starting epoch number 1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CVenv\\lib\\site-packages\\ipykernel_launcher.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384000\n",
      "768000\n",
      "Loss for Training on Epoch 1 is 1.1824396848678589\n",
      "Time for epoch 1 is 61.961254835128784\n",
      "0 10949\n",
      "1 32298\n",
      "2 26425\n",
      "3 13520\n",
      "Evaluating classifier\n",
      "Loss for validation on Epoch1 is (1.326151728630066, (array([0.        , 0.32048424, 0.45483444, 0.65066568]), array([0.        , 0.40224614, 0.39347214, 0.41033654])))\n",
      "Starting epoch number 2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CVenv\\lib\\site-packages\\ipykernel_launcher.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384000\n",
      "768000\n",
      "Loss for Training on Epoch 2 is 1.1734293699264526\n",
      "Time for epoch 2 is 61.21123433113098\n",
      "0 10949\n",
      "1 32298\n",
      "2 26425\n",
      "3 13520\n",
      "Evaluating classifier\n",
      "Loss for validation on Epoch2 is (1.322283148765564, (array([0.00136999, 0.31794538, 0.4571807 , 0.66309172]), array([0.0027243 , 0.3986413 , 0.39499109, 0.42009325])))\n",
      "Starting epoch number 3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CVenv\\lib\\site-packages\\ipykernel_launcher.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384000\n",
      "768000\n",
      "Loss for Training on Epoch 3 is 1.1676876544952393\n",
      "Time for epoch 3 is 65.29151248931885\n",
      "0 10949\n",
      "1 32298\n",
      "2 26425\n",
      "3 13520\n",
      "Evaluating classifier\n",
      "Loss for validation on Epoch3 is (1.3191254138946533, (array([0.02456845, 0.3155923 , 0.4977105 , 0.63017751]), array([0.04685187, 0.39510815, 0.4158012 , 0.42553191])))\n",
      "Starting epoch number 4\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CVenv\\lib\\site-packages\\ipykernel_launcher.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384000\n",
      "768000\n",
      "Loss for Training on Epoch 4 is 1.1642725467681885\n",
      "Time for epoch 4 is 62.1788432598114\n",
      "0 10949\n",
      "1 32298\n",
      "2 26425\n",
      "3 13520\n",
      "Evaluating classifier\n",
      "Loss for validation on Epoch4 is (1.3127814531326294, (array([0.00940725, 0.32379714, 0.51977294, 0.59889053]), array([0.01840436, 0.40006121, 0.42379549, 0.42515096])))\n"
     ]
    }
   ],
   "source": [
    "# Train the Label Classifier Model\n",
    "NUM_EPOCHS = 5\n",
    "TEST_FREQUENCY = 1\n",
    "time_start = time.time()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    time_epoch_start = time.time()\n",
    "    print(\"Starting epoch number \" + str(epoch_sum))\n",
    "    train_loss = train_classifier_cnn(train_label_dataloader, classifier, criterion, optimizer)\n",
    "    train_loss_list += [train_loss]\n",
    "    print(\"Loss for Training on Epoch \" +str(epoch_sum) + \" is \"+ str(train_loss))\n",
    "    \n",
    "    #plt.plot(train_loss_list)\n",
    "    time_epoch_end = time.time()\n",
    "    time_one_epoch = time_epoch_end - time_epoch_start\n",
    "    time_list += [time_one_epoch]\n",
    "    print(\"Time for epoch {} is {}\".format(epoch_sum,time_one_epoch))\n",
    "    \n",
    "    \n",
    "    if(epoch_sum%TEST_FREQUENCY==0):\n",
    "        val_loss= test_classifier_cnn(val_label_dataloader, classifier, criterion)\n",
    "        val_loss_list += [val_loss]\n",
    "        print('Evaluating classifier')\n",
    "        print(\"Loss for validation on Epoch\"+str(epoch_sum)+\" is \"+str(val_loss))\n",
    "    \n",
    "    epoch_sum += 1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c69a7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,\"label_classifier_mod_cd.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list1 = [loss[0] for loss in val_loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2c33490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Curve')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA42UlEQVR4nO3deVyVZfrH8c/F7oLKqoi7uC+gkkuaolauqZlZjpVmv+xXMzVtU83UtE7b5DSNv6nMzPZ02jR3S0tp0RJ33HdFRBZFUUG2+/fHfSByQFGBBw7X+/Xi5eE8nPNcB+TLc+7nfq5bjDEopZRyXx5OF6CUUqp8adArpZSb06BXSik3p0GvlFJuToNeKaXcnAa9Ukq5OQ16pZRycxr0qkoRkf0icrVD++4uIotEJF1EjonILyJyuxO1KHUxNOiVKgUR6QV8C6wEIoAg4G5gyCU+n2fZVafU+WnQK7cgIr4i8pqIJLo+XhMRX9e2YBFZUORI/HsR8XBte1REDotIhojsEJGBJeziFeB9Y8zLxphUY601xox1Pc9EEfnhnJqMiES4br8nIm+63hGcBv4sIklFA19ErheRTa7bHiLymIjsEZE0EflURALL/BunqgUNeuUuHgd6AlFAJNAdeMK17SEgAQgB6gN/AYyItAH+AFxhjPEHBgH7z31iEakJ9AI+v8wafwc8D/gDU4DTwIBztn/iun0fMAroBzQEjgOvX+b+VTWlQa/cxXjgWWNMsjEmBXgGuNW1LQcIA5oaY3KMMd8b2+QpD/AF2ouItzFmvzFmTzHPHYD9XTlymTV+ZYz50RiTb4zJAmYB4wBExB8Y6roP4C7gcWNMgjHmLPA0MEZEvC6zBlUNadArd9EQOFDk8wOu+8AOu+wGvhaRvSLyGIAxZjdwPzZEk0Vktog05L8dB/Kxfywux6FzPv8EGO0aYhoNrDPGFLyGpsAc13BTOrAN+4ep/mXWoKohDXrlLhKx4Vigies+jDEZxpiHjDEtgOuABwvG4o0xnxhj+rgea4CXz31iY8wZYBVww3n2fxqoWfCJiDQo5mt+0yrWGLMV+wdpCL8dtgH7R2GIMaZekQ8/Y8zh89SgVLE06FVV5C0ifkU+vLBDHk+ISIiIBANPAh8BiMhwEYkQEQFOYo+M80SkjYgMcB1RZwGZrm3FeQSYKCJ/EpEg1/NGishs1/aNQAcRiRIRP+y7hNL4BDse3xf4rMj904DnRaSpa18hIjKylM+p1G9o0KuqaBE2lAs+ngb+BsQBm4DNwDrXfQCtgGXAKeyR+RvGmBXY8fmXgFQgCQjFnqj9L8aYn7AnTgcAe0XkGDDdVQvGmJ3As6797AJ+KO55ijELiAG+NcakFrn/X8A87HBTBrAa6FHK51TqN0QXHlFKKfemR/RKKeXmNOiVUsrNadArpZSb06BXSik3VymvsgsODjbNmjVzugyllKoy1q5dm2qMCSluW6UM+mbNmhEXF+d0GUopVWWIyIGStunQjVJKuTkNeqWUcnMa9Eop5eYuGPQiMlNEkkUkvoTtI0Vkk4hsEJE4EelTZNt+EdlcsK0sC1dKKVU6pTmifw8YfJ7ty4FIY0wUMAmYcc72/saYKGNM9CVVqJRS6rJcMOiNMbHAsfNsP2V+bZhTi3NasSqllHJWmYzRu9a63A4sxB7VFzDY7ntrRWTyBZ5jsmvoJy4lJaUsylJKKUUZBb0xZo4xpi12jcvnimzqbYzpil1Y4fci0vc8zzHdGBNtjIkOCSl2zr9SSrmnvFzYvhB++Ge5PH2ZXjBljIkVkZYiEmyMSTXGFKzwkywic7ALNseW5T6VUqrKOrYP1n0AGz6BU0lQtwn0vAe8fMt0N5cd9CISAewxxhgR6Qr4AGkiUgvwMMZkuG5fi12YQSmlqq/cs7B9Aax9H/atBPGAiGug26vQahB4ln3Dggs+o4gUrIATLCIJwFOAN4AxZhp2Hc3bRCQHu9rPTa7Qr49d3LhgP58YY5aU+StQSqmqIGUnrHsfNs6CM2lQtzHE/AW6jIe6jcp11xcMemPMuAtsf5niF1TeC0ReemmXIOsk+NQGD70OTClVCeRkwpa5NuAPrgIPL2gzBLpOhJb9wcOzQsqolE3NLsmZYzBjIHS6EfoXu+ynUkpVjKTNdmhm06dw9gQEtoCrn4ao8VA7tMLLcZ+grxEATXrBypehYRf7V1MppSrK2QyI/8IGfOI68PSBdiOg2wRodhXYYWxHuE/Qi8Cwf8DRePhyMtz5HQRHOF2VUsqdGQOH18G69yD+S8g+BSFtYdCLEHkz1Ax0ukLAnYIewLsG3PQRvNUP/jMe/mc5+NZ2uiqllLvJTLfDMuvetweXXjWg42joOgEad3f06L047hX0APWawJiZ8NFo+OoeuPH9SvdNV0pVQcbYE6pr34etcyE3Cxp0tiMJnW4Ev7pOV1gi9wt6sGezBz4Fy56Cn6ZC7z86XZFSqqo6nWqnRK77AFJ3go8/RP3OHr03jHK6ulJxz6AHG+6J62DZ0xAWCS1inK5IKVVV5Ofbi5nWvQ/bFkB+DjTqDiNfhw7Xg08tpyu8KO4b9CL2h5KyAz67He5aaYd1lFKqOKdTIXEDJKyxR/DpB8CvHlzxP9D1Nqjf3ukKL5n7Bj2Arz/c9DG83R/+cwtMWmpP2CqlqreMJDiy0Qb7kY1wZAOcPPzr9mZXwYC/QrvrwNvPqSrLjHsHPdgplte/BbPHwcKH7FG+npxVqnowxgb4uaF+6qjrCwSCIqDplXaINyzSnmCtUc+5msuB+wc9QNuh0O9RezFVeFf7Vkwp5V6MgfSDNsiLBvuZVLtdPCC4DbTob0+ihkVCg072nb+bqx5BD9DvMUhcD4sfg/qdoEkPpytSSl0qY+DY3l+P0I9stB+Zx+12Dy8IaQetB9tAbxgF9TtUuZOoZaX6BL2HB4yeDtP7w6e32ZOz/g2crkopdSH5+XBsj+sIfYMr1DfZHjIAHt72RGm7Eb+GemgHtxhbLyvVJ+jB9sO56SN45xr4dAJMmA9ePk5XpZQ617F98Mt0G+5Jm2xrAQBPX2jQETqN+XVMPbS9/h5fQPUKerD/SUb8H3xxB3z9OAx9xemKlFJFJW2GD6+3TcLCIu3FSWFR9nZIG/D0drrCKqf6BT3Yo4HE9bDq39CwK0Sdt+W+UqqiHPwZPrnRritx1/cQ0trpitxC9V2h4+pn7FzZBffbt4dKKWft+RY+HAU1g2DSEg35MlR9g97TC8a8a/9T/edWu3CJUsoZ2+bDJzfZBTpuX6JXsZex6hv0ALVDYOyHdvX1zydBfp7TFSlV/Wz4xM6EC4uEiQvAv77TFbmd6h30AI26wdApsPc7+PY5p6tRqnpZPQ3m3m2HUW+da2fGqTJXPU/GnqvbBNvp8od/2mUI2490uiKl3JsxsPLvsOIFaDscbnhH572XIz2iLzDk7xAeDXPvgeTtTlejlPsyBpY+bkM+cpxdHEhDvlxp0Bfw8oWxH9julv8ZD1knnK5IKfeTnwfz/gCrX4fud8HIN+zECFWuNOiLqhsON75nr8qbc7e99FopVTZyz8Lnt8P6j2yTwSEv29Ykqtzpd/lczfrAtX+DHQvhh384XY1S7iH7NMwaB1u/gmufh/5/0XbhFUjfMxWn59325Oy3z9tLr1td43RFSlVdmel2jnzCL7b9SNfbnK6o2tEj+uKIwHVTbVvTL+6w7VCVUhfvVAq8PxwOr4UxMzXkHaJBXxKfmrbTJdgrZ7PPOFuPUlVN+iF4dzCk7oZxs+2i2soRFwx6EZkpIskiEl/C9pEisklENohInIj0KbJtsIjsEJHdIvJYWRZeIQKbww0z4egWmH+fnRamlLqw1N0wczCcSoZb50Crq52uqForzRH9e8Dg82xfDkQaY6KAScAMABHxBF4HhgDtgXEiUvWWUW91NQx4HDZ/Bj9Pc7oapSq/I5vskXxulm1p0LSX0xVVexcMemNMLFBixy9jzCljCg91awEFt7sDu40xe40x2cBsoGpectrnIWgzzF7ksf8Hp6tRqvI6+DO8Nxw8fWwHyrBIpytSlNEYvYhcLyLbgYXYo3qAcOBQkS9LcN1X0nNMdg39xKWkpJRFWWXHwwOuf9MO5Xw2EU4cdroipSqfgjbDtYJtyAe3croi5VImQW+MmWOMaQuMAgo6gxU3SbbEQW5jzHRjTLQxJjokJKQsyipbfnXhpo8hJ9N22ss963RFSlUeW+f92mZ4krYZrmzKdNaNa5inpYgEY4/gGxfZ3AhILMv9VbjQtjDqDTgcB4sfcboapSqH9R/DZxPsNScTF0DtUKcrUue47KAXkQgRe4mbiHQFfIA0YA3QSkSai4gPcDMw73L357j2I6HPA7D2PVj7vtPVKOWs1W/CV/dA835w21xtM1xJXfDKWBGZBcQAwSKSADwFeAMYY6YBNwC3iUgOkAnc5Do5mysifwCWAp7ATGPMlnJ5FRVtwF/tmrOLHob6HW1Pe6WqE2Ng5cuw4kVod51tM+zl63RVqgRiKuHc8OjoaBMXF+d0Ged35hi81Q9MHkxeaVerUqo6yM+Hrx+H1W9A1Hh7Fbl2oHSciKw1xkQXt02vjL1UNQPhpg/hTJrtyJeX63RFSpW/vFyYd68N+R7/CyP+rSFfBWjQX46GUTD8Ndj/PSx8EDKOOl2RUuWnoM3who+g32Mw+CVtM1xF6J/iyxU1DpI22SOcdR9A4+52abR2w+1UM6XcQfZp+M8tdq78oBeh1z1OV6Qugo7RlwVjIHkrbFsA2+dD0mZ7f/2Ov4Z+/Y7af1tVTZnp8MlYSFhj2wx3ucXpilQxzjdGr0FfHo7vd4X+Aji4GjBQr6mdndB2uD3q9/B0ukqlLuxUMnw02q6jPOYdO71YVUoa9E46lQw7Ftng37cS8rKhVgi0GWqDv3lfnZamKpf8fLtIyJY5EP8FnD0FN38EEdqBsjLToK8ssk7Crq/tkf6ubyD7FPjWgVbX2uGdiGvAt7bTVarqyBhIiLPhvnUunDwMXn52dbU+D0C4XitS2WnQV0Y5WfYIf9t8e8R/Jg08faFlfzu802Yo1ApyukrlzoyxF/5t+RK2zIUTh2zXyYirocNoaDMYfP2drlKV0vmCXmfdOMXbD1oPsh95uXBo9a/j+juXgHhAkyvtkX7b4VCv8YWfU6kLMcbOEtsyx34c3w8e3tByAPR/HNoOtQ38lFvRI/rKxhg4stEG/rYFkLLN3h8W5Qr96yCkjc7gUaVXMCss/ksb7sf2gIcXtIixy/u1HaY9atyADt1UZWl77PDO9gV2ehtAUIRr2uYIaNhFL1pRxUve7jpy/xJSd9p3ic372mGZdtfZq7uV29Cgdxcnj8COhTb49/8A+blQJxxumAFNr3S6OlUZpO76dVgmeSsg0KyPPXJvN0J7MrkxDXp3lHkcdi6F7563b8PvWa3TNKurtD2ucJ8LRzcDAk16QcfRNtz96ztdoaoAejLWHdUIgMib7Zz8j0bDj1Oh35+crkpVlOP7bbBv+dKe0wFo3MP2n2k/Euo0dLI6Vclo0Fd1EQPtL/b3U6DzjRDQzOmKVHlJP2TnuG+ZA4fX2vvCu8G1z9v/AzozS5VAg94dDHoRdi2DxY/B72Y7XY0qS7ln7dWpce/aq1XBzsC6+hk77h7Q1NHyVNWgQe8O6oZDzKPwzZOwYzG0GeJ0RepynUqBuJmwZgacTobgNjDwSWg/CoJaOl2dqmI06N1Fz3tgwyd20fLm/cCnptMVqUuRtBlWT4PNn9q+SK2utT/bFjF67YS6ZBr07sLTG4ZOgfeHww+vwoAnnK5IlVZ+np1BtfoNu4iNd03oeptdwSm4ldPVKTegQe9Oml8FncbCj/+CyHH6Fr+yO5th34WtfhOO77PXRFz9DHSboFeqqjKlQe9urv2b7ZWz6GG45Ut9u18ZHT8Av0y3K5KdPQmNutvx93bX2XdmSpUxDXp341/fNqda8ihs/Qo6jHK6IgW238zB1XZ4ZvsCQOzPpuc90KjYa1yUKjMa9O7oiv+xCzgv+bNtOas97p2Tm23nva9+A45sAL960PuPcMWddraUUhVAg94deXrBsFfhnWtg5ctw7XNOV1T9nE6DtTPhlxlwKgmCW9ufSeTN4FPL6epUNaNB764ad7eLOK9+A6J+B6HtnK6oeji6FX5+EzZ9CrlZ0HIgjHzd9nvXLqPKIRr07uzqZ2xP+4UPw8QFemK2vOTnw+5l9o/q3u/sEnyRN0OPuyG0rdPVKaVB79ZqBcPVT8GCB2DzZ9B5rNMVuZezp2DjLPh5GqTtBv8wO3um2+3a611VKhr07q7rBFj3ISx93C5bqMvEXb70Q67pke9D1glo2BVueMc2FtPpkaoS0qB3dx6eMOwf8PYA+O4FGPKy0xVVXYnr7cVoW+cBxvZ673mPPR+iw2KqErtg0IvITGA4kGyM6VjM9vHAo65PTwF3G2M2urbtBzKAPCC3pKb4qpyFd4XoSfYoNGo8hHV2uqKqJSMJlj0DGz8B37rQ6x7oPhnqNXG6MqVKpTRH9O8B/wY+KGH7PqCfMea4iAwBpgM9imzvb4xJvawq1eUb+Fd7AdXCh2DSUp0BUhq5Z+0J1tgp9nbvP8JVD4NfHacrU+qiXPC33RgTCxw7z/afjDHHXZ+uBhqVUW2qLNUIgGuetT3NN3zsdDWVmzG23fMbPWHZ09DsKvj9z/b7pyGvqqCyPqy7A1hc5HMDfC0ia0Vk8vkeKCKTRSROROJSUlLKuCwF2EZnjXvCsqfgTIl/u6u35O12acZZN9u1eG/5wi7mog3iVBVWZkEvIv2xQf9okbt7G2O6AkOA34tI35Ieb4yZboyJNsZEh4ToSvXlwsPDnpjNTIflzzpdTeWSedyu0PXmlZCw1q69evdPtoWEUlVcmQS9iHQGZgAjjTFpBfcbYxJd/yYDc4DuZbE/dRkadIQed8Ha935dd7Q6y8+zKzn9Xzc7H77rbXDfOuh5t06VVG7jsoNeRJoAXwK3GmN2Frm/loj4F9wGrgXiL3d/qgzE/Blq17cnZvPznK7GOft/hLf62QvKgtvAXbFw3Wv2QjOl3EhpplfOAmKAYBFJAJ4CvAGMMdOAJ4Eg4A2xc4kLplHWB+a47vMCPjHGLCmH16Aull8dGPQ8fHGHPbK/4g6nK6pY6Qft+rpb5kCdRjDmXbvQts6FV25KjDFO1/BfoqOjTVxcnNNluDdj4P3r7Bql966tHkex2WfsBU8/vmY/732/nTKp6+sqNyAia0u6VkknU1dXInaN2exT8M1TTldTvoyB+C/g31fAypegzVD4Qxz0/7OGvKoW3Crov96SRErGWafLqDpC20Kv39tFSg6udrqa8nFkI7w7FD6fBDUDYOIiuPFdqNfY6cqUqjBuE/THT2fz4KcbuWn6KpJOZDldTtXR9xG7KPXChyAv1+lqys7pVJj/R3uyNXUHDH8NJq+EZr2drkypCuc2QR9Qy4eZE68g+eRZxr61ioTjZ5wuqWrwrQ2DX4Sj8bDmbaeruXx5ObDqDZjaFdZ/ZKdJ3rsWom+3Dd6UqobcJugBujcP5MM7upN+Jpub3lrNgbTTTpdUNbQbYVdC+vZ528Crqtq9HN7sDUv/DI262QueBr9o2z8oVY25VdADdGkSwCd39uRMdi5j31rFnpRTTpdU+YnA0Fcg7yx8/YTT1Vy8tD3wyc22dUF+DoybDbd8CSFtnK5MqUrB7YIeoGN4XWZP7kVevuGmt1azIynD6ZIqv6CWdrrh5s9gX6zT1ZTO2Qw7H/71HrD/e7t04j2roc0QnROvVBFuGfQAbRr4M3tyLzw94Obpq4g/fMLpkiq/qx6Eek3tGrO52U5XU7L8fNjwiW1b8OO/7BKJ966DPveDl6/T1SlV6bht0ANEhNbm07t6UdPHi9+9vZr1B49f+EHVmXcNGPJ3O0tl9RtOV/PfjIE938GMgTD3brvwx53fwqg3wL++09UpVWm5ddADNA2qxX/u6klALR9ufecX1uzX9rzn1WawvaBo5ctwIsHpaixjYM+3MHMQfDgKTh2F69+CSV9DeDenq1Oq0nP7oAdoFFCT/0zuRWgdX2575xd+3K0LXp3X4JdsuC75s7N1GGNn0rxzLXx4PZw4DMNehfvWQ+TNukqWUqVUbX5TGtT14z+Te9EksCa3v7eG73YkO11S5RXQFPo+BNvmwe5lFb9/Y+x+37nGzqQ5mQjD/2nbB19xh47DK3WRqk3QA4T4+zJrck9ahdZm8gdxfL2lCs8ZL29X3gdBEbDoT5BTQVcaGwO7CgL+BjunvyDgoydpwCt1iapV0AME1vLhkzt70qFhXe75eB0LNiU6XVLl5OVr59Yf2ws/TS3ffRkDu76BGVfDxzdAxlHbsuBeDXilykK1C3qAujW8+fCO7nRpUo/7Zq3ny3WV5KRjZdNyALQfBd//A47vL/vnLwz4gfDxGDiVDNf969eWBV4+Zb9Ppaqhahn0AP5+3rw/qTu9Wgbx0GcbmfXLQadLqpwGvQDiCYsfvfDXlpYxsPNreHuADfjTKXDdVBvw3SZqwCtVxqpt0APU9PHinQlX0K91CH/+cjPv/7Tf6ZIqn7rhEPMY7FwC2xdd3nMZAzuXwtv94ZMb4UwqjPg/O0TTbYIGvFLlpFoHPYCftydv3dqNa9rX56l5W5geu8fpkiqfnndDSDt7VJ99CV1BjYEdS1wBPxbOHPs14LvepotwK1XOqn3QA/h6efLG+K4M7xzGC4u283/LdzldUuXi6Q3DpsCJg3a8vrSMgR2LYXoMzLoJMo/DiH/bIRoNeKUqzAUXB68uvD09+NfNXfDx8uAf3+zkbG4+D13bGtHmWFazPtD5JjsDJ3IcBEeU/LXG2KGeFS/aFZ4CmsHI1+3jNdyVqnAa9EV4eghTxkTi6+XBv7/bTVZOHo8Pa6dhX+Ca5+wR+uI/2TbA535fCo7gV77kCvjmMPIN23RMA14px2jQn8PDQ3jh+k74enky44d9nM3N55kRHfDw0LDHvz4MeAIWPwJbv4IOo+z9xsCORbDiJUjaZAN+1JvQaSx46n8xpZymv4XFEBGeuq49vl4evBW7l+zcfF4Y3QlPDXuIvgPWf2j74EQMhL0rbQO0pE0Q2AJGTYNON2rAK1WJ6G9jCUSEx4a0xdfbk6nLd3E2N48pN0bi5VnNz197etnGYu9cA691hsxjENhSA16pSkx/K89DRHjwmtb4ennwytIdZOfl86+bu+Bd3cO+cXfo+XvY+51dk7XjGA14pSox/e0shd/3j8DXy4O/LdxGdu46Xh/fBV8vT6fLctbgF5yuQClVStX80LT0/ueqFjw3qiPLth3lzg/Wkpmd53RJSilVKhr0F+HWnk35+w2d+X5XCpPeW8Pps7lOl6SUUhd0waAXkZkikiwi8SVsHy8im1wfP4lIZJFtg0Vkh4jsFpHHyrJwp4y9ojH/HBvFL/uPMWHmL5zMynG6JKWUOq/SHNG/Bww+z/Z9QD9jTGfgOWA6gIh4Aq8DQ4D2wDgRaX9Z1VYSo7qE8+9xXdhwKJ1bZ/xM+plsp0tSSqkSXTDojTGxQIkrahtjfjLGHHd9uhpo5LrdHdhtjNlrjMkGZgMjL7PeSmNIpzCm3dKNbUcy+N3bP5N26qzTJSmlVLHKeoz+DmCx63Y4cKjItgTXfcUSkckiEicicSkpKWVcVvm4un193p4QzZ6UU4yZtkoXHVdKVUplFvQi0h8b9AUrVBR3Gakp6fHGmOnGmGhjTHRISEhZlVXu+rUO4cM7epCbn8/4GT9z14dxHEy7hFa+SilVTsok6EWkMzADGGmMSXPdnQA0LvJljQC3XKC1e/NAvnmgH38a1Ibvd6Vy9asr+fuS7TorRylVKVx20ItIE+BL4FZjzM4im9YArUSkuYj4ADcD8y53f5WVn7cnv+8fwXcPxzA8Mow3Vuyh/5QVfLE2gfz8Et/IKKVUuRNjzh9CIjILiAGCgaPAU4A3gDFmmojMAG4ADrgekmuMiXY9dijwGuAJzDTGPF+aoqKjo01cXNzFvpZKZf3B4zw9fysbD6UT1bgeT13Xni5NApwuSynlpkRkbUH2/te2CwW9E9wh6AHy8w1zNxzmpcXbSc44y+iu4Tw6uC316/g5XZpSys2cL+j1ythy5OEhjO7aiG8fjuGemJYs2HiE/lNW8LprUROllKoIGvQVoLavF48MbsuyB/vRJyKYV5bu4Np/xrJ0SxKV8R2VUsq9aNBXoCZBNZl+WzQf3dEDP28P7vpwLbe88zM7kjKcLk0p5cY06B3Qp1Uwi+67imdHdiD+8EmG/CuWJ7+K11YKSqlyoUHvEC9PD27r1YwVD8dwa8+mfPzzQWKmrOCDVfvJzct3ujyllBvRoHdYQC0fnhnZkUX3XUX7sDo8+dUWhk79XtspKKXKjAZ9JdGmgT8f/08P3rq1G1k5tp3C5A/iOJB22unSlFJVnAZ9JSIiDOrQgK8f6Msjg9vww+5Urnk1lpeXbOeUtlNQSl0iDfpKyM/bk3tifm2n8OaKPQzQdgpKqUukQV+J1a/jx6tjo5hzz5WE1avBQ59t5Po3f2L9weMXfrBSSrlo0FcBXZoEMOfuK3l1bCRH0jO5/o2fePA/Gzh6Msvp0pRSVYAGfRVR0E7hu4dj+H3/lizY9Gs7hQxdt1YpdR7a1KyKOph2hucXbWXplqP4enlwdbv6jIhqSEybEHy9PJ0uTylVwc7X1MyrootRZaNJUE3eujWaDYfSmbMugQWbjrBw8xH8/bwY2jGMkVEN6dEiCE+P4hb6UkpVJ3pE7yZy8/L5cU8aX204zNL4JE5n5xHq78t1kQ0ZGdWQTuF1EdHQV8pdaT/6aiYrJ4/l25L5asNhVuxIITsvn+bBtRgR2ZARUQ1pGVLb6RKVUmVMg74aO3EmhyVbjjBvYyI/7UnDGOgUXpeRUQ0Z3rkhDerqIihKuQMNegXA0ZNZLNh0hHkbDrMx4QQi0KN5ICOjwhnSsQH1avo4XaJS6hJp0Kv/si/1NPM2JPLVhsPsTT2Nt6fQr3UoI6MacnW7+tTw0Zk7SlUlGvSqRMYYtiSe5KsNh5m3MZGjJ89S08eTQR0aMCKqIX0igvH21MstlKrsNOhVqeTlG37Zd4x5Gw+zcNMRTmblEljLh2Gd7HTNrk0C8NDpmkpVShr06qKdzc0jdmcqX204zLJtR8nKySe8Xg1GRNnpmm0b1HG6RKVUERr06rKcOpvLN1uTmLchkdhdqeTlG1rXr82IyIYM69yQ5sG1nC5RqWpPg16VmbRTZ1kUn8RX6w8Td8B20WwfVodhncMY1imMZhr6SjlCg16Vi8T0TBZttq0X1h9MB6BjeB2GdWrIsE5hNAmq6WyBSlUjGvSq3B1Oz2Tx5iMs2HSEDYfSAejcqC7DOoUxtFMYjQM19JUqTxr0qkIdOnaGxfFHWLjpCBsTTgAQ2bgewzuFMaRTAxoFaOgrVdY06JVjDh07w8LNNvQ3H7ahH9W4HsM72yP9hvVqOFyhUu7hsoJeRGYCw4FkY0zHYra3Bd4FugKPG2OmFNm2H8gA8oDckoo4lwa9ezqQdrow9LckngSga5N6DOvckKGdGhBWV0NfqUt1uUHfFzgFfFBC0IcCTYFRwPFigj7aGJN6MQVr0Lu/famnWeQa0992xIZ+dNMAhrmO9OvX0WZrSl2Myx66EZFmwILigr7I1zwNnNKgVxdrb8qpwtDfnpSBCFzRNJBhncMY0rEBoRr6Sl2Qk0G/DzgOGOAtY8z00hSsQV997U62ob9w0xF2HLWh372ZDf3BHRsQ6q+hr1RxnAz6hsaYRNfwzjfAvcaY2BIePxmYDNCkSZNuBw4cuGBdyr3tOprBQteR/u7kU4VtlYd2CqNb0wBahfrj46UN15QCB4P+YrYXpUf06lw7j2awYNMRFmxKZG/KaQC8PYWIUH/ah9WhfcM69t+wOtSt6e1wtUpVPEcWBxeRWoCHMSbDdfta4Nny2p9yb63r+/PgNf48cHUr9qedIf7wCbYeOcnWxJPE7krhi3UJhV8bXq9GYfB3aGj/CITXq6Fr5qpq64JBLyKzgBggWEQSgKcAbwBjzDQRaQDEAXWAfBG5H2gPBANzXL9cXsAnxpgl5fAaVDUiIjQPrkXz4FpcF9mw8P7kjCy2Hclga+JJ1x+AEyzbdpSCN6x1/Lxc4V+38I9ARGhtHfpR1YJeMKXc1pnsXHYkZRQe+W9JPMn2pJNk5eQDduinVaj/r8M+rqP/On469KOqHkeGbpRyWk0fL7o0CaBLk4DC+/LyDftSTxeG/9YjJ1mxI5nP1/469NM4sIZrvL9uYfg3rOunQz+qytKgV9WKp4cQEVqbiFDbT79AckZWkWEf++/XW38d+qlbw5v2YXW4olkAgzo2oH1YHQ1+VWXo0I1SJTiTncv2pAy2JLrCP/EEmw+fIN9Ak8CaDO7YgMEdGxDVqJ4usagcp03NlCojqafOsmzrURbHJ/HTnlRy8gwN6vgxqEN9BncM44pmAXjpYurKARr0SpWDE5k5fLv9KIs3J7FyZwpnc/MJrOXDte3rM6hjA3q3DNZZParCaNArVc7OZOeyYkcKS+KT+HZ7MqfO5uLv58XAtqEM7hhGv9Yh1PDxdLpM5cY06JWqQFk5efy4O5Ul8Ul8s+0o6WdyqOHtSUybEAZ3bMCAtqH46xROVcZ0eqVSFcjP25OB7eozsF19cvLy+WXfMRbHH2HpFju27+PpQZ9WwQzu2IBr2tUnoJaP0yUrN6dH9EpVkPx8w7qDx1kcn8SS+CQOp2fi6SH0bBHI4A4NGNRBWzKrS6dDN0pVMsYY4g+fZMmWIyyOT2JvymlEoFuTAAZ3tKGvC6qri6FBr1QlZoxhd/IpFscnsTg+qXDFrY7hdRjSMYxBHRoQEVrb4SpVZadBr1QVciDtNEvik1iyJYn1B9MBiAitTUzrEPq2DqF780D8vHUGj/otDXqlqqgjJzL5estRvtl6lF/2HSM7Lx9fLw+6Nw+kX+sQrmoVQuv6tbUdg9KgV8odZGbnsXpfGt/vTCV2Vwq7k08BUL+OL1e1skf7fSKCCdRZPNWSTq9Uyg3U8PGkf5tQ+rcJBSAxPZPvd6UQuzOVb7Ye5fO1CYhAp/C69HUFf5cm9fDWlgzVnh7RK+UG8vINmxLSid2Zyve7Ulh/KJ28fENtXy96tQyib6tg+rYOoWlQLadLVeVEh26UqmZOZOawak8asbtSiN2ZQsLxTACaBtWkb6sQrmoVTK+WQXqFrhtxi6DPyckhISGBrKwsh6pSF8vPz49GjRrh7a1h4iRjDPvTzhC704b+qr1pnMnOw8tD6NokgL6t7dF+x4Z1td1yFeYWQb9v3z78/f0JCgrSGQZVgDGGtLQ0MjIyaN68udPlqCKyc/NZe+C4Hd/flUL8YTtvP6CmN31cR/t9W4XQoK5epVuVuMXJ2KysLJo1a6YhX0WICEFBQaSkpDhdijqHj5cHvVoG0atlEI8MbkvqqbP8uDuVlTtT+H5XKvM3JgLQpr4/V7UKpnerYK5oFkht3yoTF+ocVeonpyFftejPq2oIru3LyKhwRkaFY4xhe1IGsa7Q/2DVAWb8sA9PD6Fzo7r0amH/QEQ3DdS2y1VIlQp6pVT5EhHahdWhXVgd7urXkszsPNYeOM6qvams2pPG9Ni9vLFiD96eQlTjevRqEUTPlkF0bRKgV+tWYhr0pZCWlsbAgQMBSEpKwtPTk5CQEAB++eUXfHxKvkAlLi6ODz74gKlTp5Z6f82aNSMuLo7g4ODLK1ypy1TDx5M+rYLp08r+Xzx9Npc1+4+xam8aq/ek8e/vdjP12934eHnQpXE9OyTUIoioJvXw9dLgryw06EshKCiIDRs2APD0009Tu3ZtHn744cLtubm5eHkV/62Mjo4mOrrY8yNKVTm1fL2IaRNKjOuirZNZOazZd4xVe9JYtTeNfy3fxWvLduHn7UG3pgGFQz2dG+mFW06qkkH/zPwtbE08WabP2b5hHZ66rkOpv37ixIkEBgayfv16unbtyk033cT9999PZmYmNWrU4N1336VNmzasWLGCKVOmsGDBAp5++mkOHjzI3r17OXjwIPfffz/33XdfqfZ34MABJk2aREpKCiEhIbz77rs0adKEzz77jGeeeQZPT0/q1q1LbGwsW7Zs4fbbbyc7O5v8/Hy++OILWrVqdanfGqVKVMfPu3CRFYD0M9n87Ar+1XvTmPL1TgBq+ngS3SywMPg7Nqyji6hXoCoZ9JXFzp07WbZsGZ6enpw8eZLY2Fi8vLxYtmwZf/nLX/jiiy/+6zHbt2/nu+++IyMjgzZt2nD33XeXap75H/7wB2677TYmTJjAzJkzue+++5g7dy7PPvssS5cuJTw8nPT0dACmTZvGH//4R8aPH092djZ5eXll/dKVKla9mj4Mci2iAnDsdDY/77VH+6v2pPHyku0A+Pt6cUXzX4O/XVgdPHUOf7mpkkF/MUfe5enGG2/E09OOQ544cYIJEyawa9cuRIScnJxiHzNs2DB8fX3x9fUlNDSUo0eP0qhRowvua9WqVXz55ZcA3HrrrTzyyCMA9O7dm4kTJzJ27FhGjx4NQK9evXj++edJSEhg9OjRejSvHBNYy4chncIY0ikMgJSMs6x2Bf/qPWl8uz0ZgDp+XvRoEVQY/G3q++vFW2WoSgZ9ZVGr1q99Q/7617/Sv39/5syZw/79+4mJiSn2Mb6+voW3PT09yc3NvaR9F0xdnDZtGj///DMLFy4kKiqKDRs28Lvf/Y4ePXqwcOFCBg0axIwZMxgwYMAl7UepshTi78t1kQ25LrIhAEknsmzwu8b4v9l6FLAXb/VqGURM61Bi2oYQ6q8Xb10ODfoycuLECcLDwwF47733yvz5r7zySmbPns2tt97Kxx9/TJ8+fQDYs2cPPXr0oEePHsyfP59Dhw5x4sQJWrRowX333cfevXvZtGmTBr2qlBrU9WNUl3BGdbG/O4fTM23o70njh90pLNqcBNiOnP3bhjKgbSidw7VVw8W6YNCLyExgOJBsjOlYzPa2wLtAV+BxY8yUItsGA/8CPIEZxpiXyqrwyuaRRx5hwoQJvPrqq2USqp07d8bDw56sGjt2LFOnTmXSpEm88sorhSdjAf70pz+xa9cujDEMHDiQyMhIXnrpJT766CO8vb1p0KABTz755GXXo1RFCK9XgzHdGjGmWyOMMWw9cpLvtifz7fZk/u/bXUxdvovg2j70a21Dv0+rYOrW0F5KF3LBXjci0hc4BXxQQtCHAk2BUcDxgqAXEU9gJ3ANkACsAcYZY7ZeqKjiet1s27aNdu3aleIlqcpEf26qrBw7nU3szhS+3Z7Myp0pnMjMwctD6NY0gAGuo/2I0Oq72tZl9boxxsSKSLPzbE8GkkVk2DmbugO7jTF7XUXMBkYCFwx6pZQ6V2Atn8Jhnty8fNYfSufb7cl8tz2ZFxdv58XF22kUUIMBbUPp3zaUXi2C9Gpdl/Icow8HDhX5PAHoUdIXi8hkYDJAkyZNyrEspVRV5+XpwRXNArmiWSCPDm5LYnom3+2wof9ZXAIfrDqAn7cHV7YMLhzbD69Xw+myHVOeQV/c+6cSx4mMMdOB6WCHbsqrKKWU+2lYrwbjezRlfI+mZOXksXpvmh3b32HH9/+K7cZZEPpdm9SrVhdslWfQJwCNi3zeCEgsx/0ppRR+3p6FbRqeNoY9KacLT+jO+H4v01buoW4Nb/q2DqF/mxBi2oS6/YLq5Rn0a4BWItIcOAzcDPyuHPenlFK/ISJEhNYmIrQ2d/ZtwcmsHH7Ylcq325NZsSOF+RsTEYGoxvUY0MaO7XdoWMftTuiWZnrlLCAGCBaRBOApwBvAGDNNRBoAcUAdIF9E7gfaG2NOisgfgKXY6ZUzjTFbyuVVKKVUKdTx82ZopzCGdgojP98Qn3ii8ITuP77ZyT++2Un9Or70iQihd0QQvSOCqV+n6l+sdcFBKmPMOGNMmDHG2xjTyBjzjjFmmjFmmmt7kuv+OsaYeq7bJ13bFhljWhtjWhpjni/vF1OeYmJiWLp06W/ue+2117jnnnvO+5iCaaJDhw4t7EVT1NNPP82UKVP+6/6i5s6dy9atv05WevLJJ1m2bNlFVF+8FStWMHz48Mt+HqWqIg8PoXOjetx/dWu++kMf1jx+Na+M6Ux000C+3X6UBz/dSI8XljPwHyt48qt4lsQnceJM8a1NKju9MraUxo0bx+zZsxk0aFDhfbNnz+aVV14p1eMXLVp0yfueO3cuw4cPp3379gA8++yzl/xcSqnihfj7cmN0Y26Mbkx+vr1Y66c9qfy4O61wJo+HQMfwulzZMpjeEVVnpa2qGfSLH4OkzWX7nA06wZCSL9wdM2YMTzzxBGfPnsXX15f9+/eTmJhInz59uPvuu1mzZg2ZmZmMGTOGZ5555r8eX3Qxkeeff54PPviAxo0bExISQrdu3QB4++23mT59OtnZ2URERPDhhx+yYcMG5s2bx8qVK/nb3/7GF198wXPPPcfw4cMZM2YMy5cv5+GHHyY3N5crrriCN998E19fX5o1a8aECROYP38+OTk5fPbZZ7Rt27ZU34pZs2bxwgsvYIxh2LBhvPzyy+Tl5XHHHXcQFxeHiDBp0iQeeOABpk6dyrRp0/Dy8qJ9+/bMnj370r7/SlUiHh5Cx/C6dAyvy+S+LcnOzWfDoXR+3J3KT3tSC0/q+nh60LVpPXq3DObKiGAiG9WtlLN5qmbQOyAoKIju3buzZMkSRo4cyezZs7npppsQEZ5//nkCAwPJy8tj4MCBbNq0ic6dOxf7PGvXrmX27NmsX7+e3NxcunbtWhj0o0eP5s477wTgiSee4J133uHee+9lxIgRhcFeVFZWFhMnTmT58uW0bt2a2267jTfffJP7778fgODgYNatW8cbb7zBlClTmDFjxgVfZ2JiIo8++ihr164lICCAa6+9lrlz59K4cWMOHz5MfHw8QOEw1EsvvcS+ffvw9fUtdmhKKXfg4+VB9+aBdG8eyAPXtOb02Vx+2X+Mn3bbI/6C8f3avl70aB7IlRH2iL9Nff9KcWK3agb9eY68y1PB8E1B0M+cOROATz/9lOnTp5Obm8uRI0fYunVriUH//fffc/3111OzZk0ARowYUbgtPj6eJ554gvT0dE6dOvWbYaLi7Nixg+bNm9O6dWsAJkyYwOuvv14Y9AVti7t161bY4vhC1qxZQ0xMTOFSiePHjyc2Npa//vWv7N27l3vvvZdhw4Zx7bXXArYnz/jx4xk1ahSjRo0q1T6Uqupq+XrRv00o/V0rbR07nc2qPWn8uCeVn3anstzVfjm4tg+9WgbTu6U9sds4sKYj9VbNoHfIqFGjePDBB1m3bh2ZmZl07dqVffv2MWXKFNasWUNAQAATJ04kKyvrvM9T0l/4iRMnMnfuXCIjI3nvvfdYsWLFeZ/nQn2KCloiX0w75JKeMyAggI0bN7J06VJef/11Pv30U2bOnMnChQuJjY1l3rx5PPfcc2zZsqXEZRWVcleBtXwY1jmMYZ1t3/3D6Zl2mGd3Kj/uSWP+RnsJUePAGoXDPFe2DCK4tu/5nrbMVL7BpEqsdu3axMTEMGnSJMaNGwfAyZMnqVWrFnXr1uXo0aMsXrz4vM/Rt29f5syZQ2ZmJhkZGcyfP79wW0ZGBmFhYeTk5PDxxx8X3u/v709GRsZ/PVfbtm3Zv38/u3fvBuDDDz+kX79+l/Uae/TowcqVK0lNTSUvL49Zs2bRr18/UlNTyc/P54YbbuC5555j3bp15Ofnc+jQIfr378/f//73wnciSlV34fVqMDa6Ma/d3IVf/jKQbx7oy9PXtadtgzos3HyE+2atJ/pvyxj8WizPzt/K8m1Hycgqvxk9euh1kcaNG8fo0aMLTzpGRkbSpUsXOnToQIsWLejdu/d5H1+wvmxUVBRNmzblqquuKtz23HPP0aNHD5o2bUqnTp0Kw/3mm2/mzjvvZOrUqXz++eeFX+/n58e7777LjTfeWHgy9n//938v6vUsX778NytcffbZZ7z44ov0798fYwxDhw5l5MiRbNy4kdtvv538/HwAXnzxRfLy8rjllls4ceIExhgeeOAB6tWrd1H7V8rdiQit6vvTqr4/E3s3Jzcvn/jEk4Undj/6+QAzf9yHp4fQrUkAsyb3LPNlFS/YptgJ2qbYfejPTanzy8rJY92B4/y4J5W0U9m8dEPx5/cu5LLaFCullCo/ft6edsw+Irjc9qFj9Eop5eaqVNBXxmEmVTL9eSlVOVSZoPfz8yMtLU3Do4owxpCWloafX9VvCKVUVVdlxugbNWpEQkICKSkpTpeiSsnPz+83M3qUUs6oMkHv7e1N8+bNnS5DKaWqnCozdKOUUurSaNArpZSb06BXSik3VymvjBWRFODAJT48GEgtw3KqAn3N7q+6vV7Q13yxmhpjQorbUCmD/nKISFxJlwG7K33N7q+6vV7Q11yWdOhGKaXcnAa9Ukq5OXcM+ulOF+AAfc3ur7q9XtDXXGbcboxeKaXUb7njEb1SSqkiNOiVUsrNuU3Qi8hgEdkhIrtF5DGn6ylvItJYRL4TkW0iskVE/uh0TRVFRDxFZL2ILHC6loogIvVE5HMR2e76efdyuqbyJiIPuP5fx4vILBFxuzaoIjJTRJJFJL7IfYEi8o2I7HL9G1AW+3KLoBcRT+B1YAjQHhgnIu2drarc5QIPGWPaAT2B31eD11zgj8A2p4uoQP8Clhhj2gKRuPlrF5Fw4D4g2hjTEfAEbna2qnLxHjD4nPseA5YbY1oBy12fXza3CHqgO7DbGLPXGJMNzAZGOlxTuTLGHDHGrHPdzsD+8oc7W1X5E5FGwDBghtO1VAQRqQP0Bd4BMMZkG2PSHS2qYngBNUTEC6gJJDpcT5kzxsQCx865eyTwvuv2+8CostiXuwR9OHCoyOcJVIPQKyAizYAuwM8Ol1IRXgMeAfIdrqOitABSgHddw1UzRKSW00WVJ2PMYWAKcBA4ApwwxnztbFUVpr4x5gjYgzkgtCye1F2CXoq5r1rMGxWR2sAXwP3GmJNO11OeRGQ4kGyMWet0LRXIC+gKvGmM6QKcpozezldWrnHpkUBzoCFQS0Rucbaqqs1dgj4BaFzk80a44Vu9c4mINzbkPzbGfOl0PRWgNzBCRPZjh+cGiMhHzpZU7hKABGNMwbu1z7HB786uBvYZY1KMMTnAl8CVDtdUUY6KSBiA69/ksnhSdwn6NUArEWkuIj7YEzfzHK6pXImIYMdttxljXnW6nopgjPmzMaaRMaYZ9mf8rTHGrY/0jDFJwCERaeO6ayCw1cGSKsJBoKeI1HT9Px+Im5+ALmIeMMF1ewLwVVk8aZVZSvB8jDG5IvIHYCn2DP1MY8wWh8sqb72BW4HNIrLBdd9fjDGLnCtJlZN7gY9dBzF7gdsdrqdcGWN+FpHPgXXY2WXrccN2CCIyC4gBgkUkAXgKeAn4VETuwP7Bu7FM9qUtEJRSyr25y9CNUkqpEmjQK6WUm9OgV0opN6dBr5RSbk6DXiml3JwGvVJKuTkNeqWUcnP/D5LwC5QDMgXQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list,label=\"Train Loss\")\n",
    "plt.plot(val_loss_list1,label=\"Validation Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d61f3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier,\"Only_MLP_\"+\"epoch{}.pt\".format(epoch_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb69160",
   "metadata": {},
   "source": [
    "### Predict with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d2c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_se(model, X_test, y_test, dataset_SE_max, plot=True):\n",
    "    print(\"[INFO] predicting...\")\n",
    "    X_test = torch.tensor(X_test).to(device)\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.cpu().detach().numpy().flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    diff = abs(preds-y_test)*dataset_SE_max\n",
    "    # choose how many bins you want here\n",
    "    num_bins = 30\n",
    "    # use the histogram function to bin the data\n",
    "    counts, bin_edges = np.histogram(diff, bins=num_bins)\n",
    "    # now find the cdf\n",
    "    cdf = np.cumsum(counts)\n",
    "    # and finally plot the cdf\n",
    "    if plot:\n",
    "        plt.plot(bin_edges[1:], cdf)\n",
    "        plt.xlabel('Absolute Error (Mbit/Number)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Error CDF')\n",
    "        plt.show()\n",
    "    diff_percen = abs(preds-y_test)/y_test\n",
    "    mean = np.mean(abs(diff))\n",
    "    std = np.std(diff)\n",
    "    print(\"[INFO] error mean: {:.3f}, std: {:.3f}\".format(mean, std))\n",
    "    mean = np.mean(abs(diff_percen))\n",
    "    std = np.std(diff_percen)\n",
    "    print(\"[INFO] percentage error mean: {:.3f}, std: {:.3f}\" .format(mean, std))\n",
    "    axis_lim = y_test.mean()*dataset_SE_max*2\n",
    "    # mean absolute percentage error by sections\n",
    "    diff_with_y = np.hstack((y_test[:,np.newaxis]*dataset_SE_max, diff_percen[:,np.newaxis]))\n",
    "    sort_indices = np.argsort(diff_with_y[:,0])\n",
    "    diff_with_y = diff_with_y[sort_indices,:]\n",
    "    mape_sections = np.array([1000, 1500, 2500, np.iinfo(int).max])\n",
    "    mape_sections_mean = np.empty(len(mape_sections))\n",
    "    idx_section = 0\n",
    "    diff_id_section = 0\n",
    "    for i in range(len(diff_with_y)):\n",
    "        if diff_with_y[i,0]>mape_sections[idx_section] or i == len(diff_with_y)-1:\n",
    "            mape_sections_mean[idx_section] = np.mean(diff_with_y[diff_id_section:i,1])\n",
    "            diff_id_section = i\n",
    "            idx_section = idx_section + 1\n",
    "    print(\"[INFO] MAPE by sections: \", np.around(mape_sections_mean, decimals=2))\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(preds.flatten()*dataset_SE_max, y_test*dataset_SE_max, '.')\n",
    "        plt.plot([0, axis_lim], [0, axis_lim])\n",
    "        plt.axis([0, axis_lim, 0, axis_lim])\n",
    "        plt.xlabel('Prediction (Mbit/Number)')\n",
    "        plt.ylabel('Ground Truth (Mbit/Number)')\n",
    "        plt.title('Spectral Efficiency')\n",
    "        plt.show()\n",
    "\n",
    "def VisPredict(model,plotOn,X_train,y_train,X_val,y_val,X_test,y_test,dataset_SE_max):\n",
    "    # make predictions on the testing data\n",
    "    predict_se(model, X_train, y_train, dataset_SE_max, plotOn)\n",
    "    predict_se(model, X_val, y_val, dataset_SE_max, plotOn)\n",
    "    predict_se(model, X_test, y_test, dataset_SE_max, plotOn)\n",
    "    if plotOn:\n",
    "        dot_img_file = 'figures/model_1.png'\n",
    "        #tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89d68a",
   "metadata": {},
   "source": [
    "### Predict Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f719b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9da14173e49b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mVisPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache ()\n",
    "VisPredict(model,True,X_train,y_train,X_val,y_val,X_test,y_test,y_max[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6773c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee26ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbf804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5146f3029d6424cdc6ef191d54c22462012605892d6b80a61222a44e90cce02"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
